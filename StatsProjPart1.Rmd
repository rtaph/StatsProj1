---
title: "Statistical Inference: Course Project"
author: "rtaph"
output: pdf_document
---

```{r echo=FALSE}
  # Set working directory
  setwd("~/datasciencecoursera/StatsProj1/")
  library(ggplot2)
```
# Part I: The Exponential Distribution
To explore the exponential distribution, we begin by making a histogram of an experiment where we draw 40 random variables $X$ from the distribution. For our purposes, we will use the exponential distribution with $\lambda = 0.2$, but any value of lambda greater than zero can be chosen. We can get a sense of what distribution looks like by plotting a histogram of this experiment:

```{r}
set.seed(5678)
n = 40; lambda = 0.2;
experiment1 = rexp(n, lambda)
hist(experiment1)
```

In our experiment, the mean of the `r n` draws is calculated to be `r mean(experiment1)`. Given the inherent variability in random data, we might expect this sample mean ($\bar{x}$) to differ slightly from the population mean from which it was drawn ($\mu$).

To investigate this further, we turn to asymptotics by repeating the experiment many times through a monte carlo simulation. The means of each experiment are saved in a vector object: 

```{r}
monte.carlo.means = NULL; m = 1000;
for (i in 1 : m) monte.carlo.means = c(monte.carlo.means, mean(rexp(n, lambda)))
```

## 1. Show where the distribution is centered at and compare it to the theoretical center of the distribution.

The expected value of an exponentially distributed random variable $X$ with rate parameter $\lambda$ is given by $E[X] = \frac{1}{\lambda}$. For the distribution chosen in this paper, the theoretical mean evaluates to $1/ `r lambda` = `r 1/lambda`$.

By plotting the cumulative mean for each additional experiment (in black), we can see that the monte carlo mean asymptotes to the theoretical mean (in red).

```{r}
theoretical.mean = (1/lambda)
cumulative.means <- cumsum(monte.carlo.means) / (1 : m)
plot(cumulative.means, type = "l", lwd = 3)
abline(h=theoretical.mean, col = "red", lwd = 3)
legend("topright", legend = c("Monte Carlo Cumulative Mean", "Theoretical Mean"), 
       lty = 1:2, lwd=c(3,3), col=c("black","red"))
```

As we would expect, the cumulative mean from our repeated experiement evaluates to `cumulative.means[``r m``] = ``r cumulative.means[m]`. This value is much closer to the theoretical mean of `r 1/lambda` than our single experiment mean of `r mean(experiment1)`.

## 2. Show how variable it is and compare it to the theoretical variance of the distribution.

The theretical variance of the distribution is given by the expression $Var[X] = \frac{1}{\lambda^2}$. We compare both the theoretical and empirical results below:

```{r}
theoretical.var = 1/(lambda^2)
empirical.var = var(experiment1)
c(Theoretical = theoretical.var, Experiment1 = empirical.var)
```

These two values are close. We would expect the culumulative variance of multiple experiments to converge to the theoretical variance as a result of the Law of Large Numbers.

## 3. Show that the distribution is approximately normal.

The Central Limit Theorem (CLT) tells us that the resulting distribution of means from our `r m` experiments should tend towards normality. We can visually test this on the basis of our empirical data by plotting a histogram of the means and producing a QQ plot.
```{r}
par(mfrow=c(1,2))
hist(monte.carlo.means); 
x<-seq(min(monte.carlo.means),max(monte.carlo.means),0.1) 
curve(dnorm(x, theoretical.mean, sqrt(theoretical.var)), col="green", lwd=3, add=TRUE, yaxt="n")
qqnorm(monte.carlo.means); qqline(monte.carlo.means, col="green", lwd=3)
```

The green line represents the identity line for normal distribution. Although the distribution is not perfectly normal, visual inspection reveals that the fit is pretty good. The deviations from normality fall within the expected range of noise for a monte carlo simulation. 


